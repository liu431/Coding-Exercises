## Notes 

Taken from [Udemy class Spark and Python for Big Data with PySpark](https://www.udemy.com/course/spark-and-python-for-big-data-with-pyspark/)

Skills: Spark, PySpark, Spark Streaming, MLlib for Machine Learning, Spark 2.0 DataFrames, DataBricks 

### Spark and Big Data basics
#### Local vs. Distributed
* Local: limited to the cores/computation resoruces on local machine
* Distributed: has access to the computational resoruces across a number of machines connected throigh a network; easily scaling; fault tolerance

#### Hadoop
* A way to distribute very large files across multiple machines
* HDFS: Hadoop Distributed File System
* Allow a user to work with large data sets; duplicate blocks of data for fault tolerance
* MapReduce: allow computation on that data

### Setting up Spark in various ways

### Python and Spark 2.0 DataFrames

### PySpark Project Exercise

### Introduction to Machine Learning

### Linear Regression
 
### Logistic Regression

### Decision Trees and Random Forests

### Gradient Boosted Trees

### K-means Clustering

### Recommender Systems

### Natural Language Processing


### Spark Streaming (Local and Twitter)
